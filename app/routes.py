from __future__ import annotations

from datetime import datetime
from typing import Any, Dict, List, Optional

import pandas as pd
from fastapi import File, Query, Request, UploadFile
from fastapi.responses import HTMLResponse, RedirectResponse

from app.preprocessing import clean_column_values

from .analytics import analyze_dataset, prediction_summary, transform_single_record
from .records_statistis import load_and_create_stats_graphs
from .csv_utils import read_uploaded_dataframe
from .forms import build_context, parse_form_to_record
from .supabase_service import ingest_dataset, save_request_with_prediction
import logging
logger = logging.getLogger(__name__)

from fastapi.responses import FileResponse
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import hashlib
import os
import tempfile
from bs4 import BeautifulSoup

def build_vertical_preview_html(dataframe: pd.DataFrame) -> str:
    if dataframe is None or dataframe.empty:
        return '<table class="preview-table preview-table--vertical"></table>'

    row = dataframe.head(1).T.reset_index()
    row.columns = ["–ü—Ä–∏–∑–Ω–∞–∫", "–ó–Ω–∞—á–µ–Ω–∏–µ"]

    def format_label(label: Any) -> str:
        text = str(label).replace("_", " ").strip()
        return text or "-"

    def format_value(value: Any) -> str:
        if pd.isna(value):
            return "-"
        if isinstance(value, float):
            return f"{value:.6g}"
        return str(value)

    row["–ü—Ä–∏–∑–Ω–∞–∫"] = row["–ü—Ä–∏–∑–Ω–∞–∫"].map(format_label)
    row["–ó–Ω–∞—á–µ–Ω–∏–µ"] = row["–ó–Ω–∞—á–µ–Ω–∏–µ"].map(format_value)

    return row.to_html(index=False, classes=["preview-table", "preview-table--vertical"], border=0)

def register_routes(app) -> None:
    templates = app.state.templates

    @app.get("/", response_class=HTMLResponse)
    async def upload_page(request: Request) -> HTMLResponse:
        store = app.state.store
        try:
            context = store.ensure_context()
        except RuntimeError as exc:
            return templates.TemplateResponse(
                "index.html",
                {
                    "request": request,
                    "result": None,
                    "error": str(exc),
                    "notifications": [],
                    "filename": None,
                    "model_ready": False,
                    "prediction": None,
                    "prediction_error": None,
                    "input_metadata": [],
                    "importance_image": None,
                    "top_features": [],
                    "rf_visuals": {},
                },
            )

        notifications = store.last_analysis["notifications"] if store.last_analysis else []
        
        return templates.TemplateResponse(
            "index.html",
            build_context(
                request,
                store=store,
                context=context,
                result=store.last_analysis,
                error=None,
                notifications=notifications,
            ),
        )

    
    @app.get("/analyze")
    async def reroute_get_analyze():
        return RedirectResponse(url="/", status_code=302)


    @app.post("/analyze", response_class=HTMLResponse)
    async def analyze_csv(request: Request, file: UploadFile = File(...)) -> HTMLResponse:
        store = app.state.store
        templates = app.state.templates
        try:
            context = store.ensure_context()
        except RuntimeError as exc:
            return templates.TemplateResponse(
                "index.html",
                {
                    "request": request,
                    "result": store.last_analysis,
                    "error": str(exc),
                    "notifications": store.last_analysis["notifications"] if store.last_analysis else [],
                    "filename": None,
                    "model_ready": False,
                    "prediction": None,
                    "prediction_error": None,
                    "input_metadata": [],
                    "importance_image": None,
                    "top_features": [],
                    "rf_visuals": {},
                },
            )

        if not file.filename.lower().endswith(".csv"):
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error="–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV.",
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                ),
            )

        content = await file.read()
        file_hash = hashlib.sha256(content).hexdigest()

        try:
            dataframe = read_uploaded_dataframe(content)
            dataframe.attrs["source_filename"] = file.filename
        except Exception as exc:
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å CSV: {exc}",
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                ),
            )
        filenames: List[str] = []
        supabase_warning: Optional[str] = None
        client = store.supabase_client
        table_name = store.settings.supabase_table

        if client is not None and table_name:
            try:
                if "." in table_name:
                    schema_name, table_name_only = table_name.split(".", maxsplit=1)
                    table_query = client.schema(schema_name).table(table_name_only)
                else:
                    table_query = client.table(table_name)
                response = table_query.select("filename").execute()
                select_result = getattr(response, "data", None) or []
                filenames = [
                    item.get("filename")
                    for item in select_result
                    if isinstance(item, dict) and item.get("filename")
                ]
                logger.info("–§–∞–π–ª %s –Ω–∞–π–¥–µ–Ω –≤ Supabase —Ç–∞–±–ª–∏—Ü–µ %s.", file.filename, table_name)
            except Exception as exc:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü–µ %s: %s", table_name, exc)
                supabase_warning = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ Supabase: {exc}"
        else:
            logger.info("Supabase –∫–ª–∏–µ–Ω—Ç –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–æ–≤.")

        def merge_notifications(base_notifications: Optional[List[str]]) -> List[str]:
            merged = list(base_notifications or [])
            if supabase_warning:
                merged.append(supabase_warning)
            return merged

        cached_entry = store.dataset_cache.get(file_hash)
        if cached_entry:
            store.last_analysis = cached_entry["analysis"]
            store.current_metadata = cached_entry["metadata"]
            store.last_filename = file.filename
            store.filename_to_hash[file.filename] = file_hash
            cached_entry["filename"] = file.filename

            cached_notifications = cached_entry.get("notifications")
            notifications_list = merge_notifications(cached_notifications)
            notifications_list.append("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ä–∞–Ω–µ–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª ‚Äî –ø–æ–∫–∞–∑–∞–Ω—ã —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.")

            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=cached_entry["analysis"],
                    error=None,
                    notifications=notifications_list,
                    filename=file.filename,
                ),
            )

        skip_ingest = file.filename in filenames

        try:
            analysis, storage_df = analyze_dataset(dataframe, context, store)
        except Exception as exc:
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö: {exc}",
                    notifications=merge_notifications(
                        store.last_analysis["notifications"] if store.last_analysis else []
                    ),
                ),
            )

        if not skip_ingest:
            ingest_dataset(store, storage_df, file.filename, source="csv")
        else:
            logger.info("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –≤ Supabase –¥–ª—è —Ñ–∞–π–ª–∞ %s: —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.", file.filename)

        store.last_analysis = analysis
        store.current_metadata = analysis["metadata"]
        store.last_filename = file.filename
        store.dataset_cache[file_hash] = {
            "analysis": analysis,
            "metadata": analysis["metadata"],
            "filename": file.filename,
            "notifications": analysis.get("notifications"),
        }
        store.filename_to_hash[file.filename] = file_hash

        notifications = merge_notifications(analysis.get("notifications"))
        if skip_ingest:
            notifications.append("–§–∞–π–ª —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º —É–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏.")

        return templates.TemplateResponse(
            "index.html",
            build_context(
                request,
                store=store,
                context=context,
                result=analysis,
                error=None,
                notifications=notifications,
                filename=file.filename,
            ),
        )

    @app.post("/predict", response_class=HTMLResponse)
    async def predict_single(request: Request, file: UploadFile = File(...)) -> HTMLResponse:
        store = app.state.store
        templates = app.state.templates
        try:
            context = store.ensure_context()
        except RuntimeError as exc:
            return templates.TemplateResponse(
                "index.html",
                {
                    "request": request,
                    "result": store.last_analysis,
                    "error": str(exc),
                    "notifications": store.last_analysis["notifications"] if store.last_analysis else [],
                    "filename": None,
                    "model_ready": False,
                    "prediction": None,
                    "prediction_error": None,
                    "input_metadata": [],
                    "importance_image": None,
                    "top_features": [],
                    "rf_visuals": {},
                },
            )

        if not file.filename.lower().endswith(".csv"):
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=None,
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                    prediction=None,
                    prediction_error="–î–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π.",
                ),
            )

        content = await file.read()
        try:
            dataframe = read_uploaded_dataframe(content)
            dataframe.attrs["source_filename"] = file.filename
        except Exception as exc:
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=None,
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                    prediction=None,
                    prediction_error=f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å CSV –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞: {exc}",
                ),
            )

        if dataframe.empty or len(dataframe) != 1:
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=None,
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                    prediction=None,
                    prediction_error="CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ä–æ–≤–Ω–æ –æ–¥–Ω—É –∑–∞–ø–∏—Å—å.",
                ),
            )
        dataframe, clean_notifications = clean_column_values(dataframe)
        try:
            features, prep_notifications = transform_single_record(dataframe, context)
        except Exception as exc:
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=None,
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                    prediction=None,
                    prediction_error=f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∑–∞–ø–∏—Å—å: {exc}",
                ),
            )

        prediction = prediction_summary(features, context)
        prediction["preview_html"] = dataframe.to_html(index=False, classes="preview-table")
        prediction["preview_vertical_html"] = build_vertical_preview_html(dataframe)
        prediction["preprocessing_notes"] = prep_notifications
        prediction["source"] = "csv"
        prediction["generated_at"] = datetime.utcnow().isoformat()
        store.last_prediction = prediction

        storage_df = dataframe.copy()
        storage_df["predicted_anomaly"] = prediction["anomaly_label"]
        storage_df["is_suspicious"] = int(prediction["suspicious"])
        storage_df["analysis_run_at"] = datetime.utcnow().isoformat()
        storage_df["source_filename"] = file.filename
        # ingest_dataset(store, storage_df, file.filename, source="single_csv_prediction")

        record_to_save = dataframe.iloc[0].to_dict()
        record_to_save["predicted_class"] = prediction["anomaly_label"]
        record_to_save["class_probability"] = int(prediction["suspicious"])
        
        try:
            save_request_with_prediction(store,record_to_save)
        except Exception as e:
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=None,
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                    prediction=prediction,
                    prediction_error=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}",
                ),
            )
        return templates.TemplateResponse(
            "index.html",
            build_context(
                request,
                store=store,
                context=context,
                result=store.last_analysis,
                error=None,
                notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                prediction=prediction,
                prediction_error=None,
            ),
        )

    @app.post("/predict-form", response_class=HTMLResponse)
    async def predict_from_form(request: Request) -> HTMLResponse:
        store = app.state.store
        templates = app.state.templates
        try:
            context = store.ensure_context()
        except RuntimeError as exc:
            return templates.TemplateResponse(
                "index.html",
                {
                    "request": request,
                    "result": store.last_analysis,
                    "error": str(exc),
                    "notifications": store.last_analysis["notifications"] if store.last_analysis else [],
                    "filename": None,
                    "model_ready": False,
                    "prediction": None,
                    "prediction_error": None,
                    "input_metadata": [],
                    "importance_image": None,
                    "top_features": [],
                    "rf_visuals": {},
                },
            )

        form_data = await request.form()
        form_dict = dict(form_data)

        metadata = store.current_metadata or store.generate_metadata(None)

        try:
            record = parse_form_to_record(form_dict, metadata)
        except Exception as exc:
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=None,
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                    prediction=None,
                    prediction_error=f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è —Ñ–æ—Ä–º—ã: {exc}",
                    form_values=form_dict,
                ),
            )
        
        record_df = pd.DataFrame([record])
        record_df, clean_notifications = clean_column_values(record_df)
        try:
            features, prep_notifications = transform_single_record(record_df, context)
        except Exception as exc:
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=None,
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                    prediction=None,
                    prediction_error=f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∑–∞–ø–∏—Å—å: {exc}",
                    form_values=form_dict,
                ),
            )

        prediction = prediction_summary(features, context)
        prediction["preview_html"] = record_df.to_html(index=False, classes="preview-table")
        prediction["preview_vertical_html"] = build_vertical_preview_html(record_df)
        prediction["preprocessing_notes"] = prep_notifications
        prediction["source"] = "form"
        prediction["generated_at"] = datetime.utcnow().isoformat()
        store.last_prediction = prediction

        storage_df = record_df.copy()
        storage_df["predicted_anomaly"] = prediction["anomaly_label"]
        storage_df["is_suspicious"] = int(prediction["suspicious"])
        storage_df["analysis_run_at"] = datetime.utcnow().isoformat()
        storage_df["source_filename"] = "form_submission"
        # ingest_dataset(store, storage_df, "form_submission", source="form_submission")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –≤ Supabase (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ /predict)
        record_to_save = record_df.iloc[0].to_dict()
        record_to_save["predicted_class"] = prediction["anomaly_label"]
        record_to_save["class_probability"] = int(prediction["suspicious"])

        try:
            save_request_with_prediction(store, record_to_save)
        except Exception as e:
            return templates.TemplateResponse(
                "index.html",
                build_context(
                    request,
                    store=store,
                    context=context,
                    result=store.last_analysis,
                    error=None,
                    notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                    prediction=prediction,
                    prediction_error=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}",
                    form_values=form_dict,
                ),
            )
        
        return templates.TemplateResponse(
            "index.html",
            build_context(
                request,
                store=store,
                context=context,
                result=store.last_analysis,
                error=None,
                notifications=store.last_analysis["notifications"] if store.last_analysis else [],
                prediction=prediction,
                prediction_error=None,
                form_values=form_dict,
            ),
        )

    MAX_CELL_LENGTH = 50  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —è—á–µ–π–∫–µ

    def truncate_text(text: str, max_len: int = MAX_CELL_LENGTH) -> str:
        if len(text) > max_len:
            return text[: max_len - 3] + "..."
        return text

    def ensure_pdf_font() -> Optional[HTMLResponse]:
        font_path = os.path.join(os.path.dirname(__file__), "fonts", "DejaVuSans.ttf")
        if not os.path.exists(font_path):
            return HTMLResponse(
                "<h3>–§–∞–π–ª —à—Ä–∏—Ñ—Ç–∞ DejaVuSans.ttf –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–º–µ—Å—Ç–∏—Ç–µ –µ–≥–æ –≤ –ø–∞–ø–∫—É /fonts —Ä—è–¥–æ–º —Å routes.py</h3>"
            )
        if "DejaVuSans" not in pdfmetrics.getRegisteredFontNames():
            pdfmetrics.registerFont(TTFont("DejaVuSans", font_path))
        return None

    def build_dataset_story(result: Dict[str, Any], styles, available_width: float) -> List[Any]:
        story: List[Any] = []
        story.append(Paragraph("üìä –û—Ç—á—ë—Ç –ø–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É", styles["TitleRu"]))
        story.append(Spacer(1, 16))

        story.append(Paragraph("–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞:", styles["Heading"]))
        metrics_rows = [
            ["–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å", "–ó–Ω–∞—á–µ–Ω–∏–µ"],
            ["–û–±—ä—ë–º —Å—Ç—Ä–æ–∫", truncate_text(str(result.get("records", "-")))],
            ["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤", truncate_text(str(result.get("columns", "-")))],
        ]
        if result.get("prediction_counts"):
            metrics_rows.append(["–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –º–æ–¥–µ–ª–∏", truncate_text(str(len(result["prediction_counts"])))])

        metrics_table = Table(
            [[Paragraph(str(cell), styles["Body"]) for cell in row] for row in metrics_rows],
            hAlign="LEFT",
            colWidths=[200, 250],
        )
        metrics_table.setStyle(
            TableStyle(
                [
                    ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                    ("BACKGROUND", (0, 0), (-1, 0), colors.lightblue),
                    ("ALIGN", (0, 0), (-1, 0), "CENTER"),
                    ("VALIGN", (0, 0), (-1, -1), "TOP"),
                ]
            )
        )
        story.append(metrics_table)
        story.append(Spacer(1, 12))

        preview_html = result.get("preview_html")
        if preview_html:
            story.append(Paragraph("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫):", styles["Heading"]))
            soup = BeautifulSoup(preview_html, "html.parser")
            rows = soup.find_all("tr")
            table_data: List[List[Paragraph]] = []
            for row in rows[:11]:
                cols = [truncate_text(cell.get_text(strip=True)) for cell in row.find_all(["th", "td"])]
                table_data.append([Paragraph(col, styles["Body"]) for col in cols])

            if table_data:
                num_cols = len(table_data[0]) or 1
                col_width = max(40, available_width / num_cols)
                total_width = col_width * num_cols
                if total_width > available_width:
                    col_width = available_width / num_cols
                col_widths = [col_width] * num_cols
                preview_table = Table(table_data, repeatRows=1, colWidths=col_widths)
                preview_table.setStyle(
                    TableStyle(
                        [
                            ("GRID", (0, 0), (-1, -1), 0.25, colors.grey),
                            ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
                            ("VALIGN", (0, 0), (-1, -1), "TOP"),
                            ("LEFTPADDING", (0, 0), (-1, -1), 2),
                            ("RIGHTPADDING", (0, 0), (-1, -1), 2),
                            ("TOPPADDING", (0, 0), (-1, -1), 2),
                            ("BOTTOMPADDING", (0, 0), (-1, -1), 2),
                        ]
                    )
                )
                story.append(preview_table)
                story.append(Spacer(1, 12))

        if result.get("prediction_counts"):
            story.append(PageBreak())
            story.append(Paragraph("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –º–æ–¥–µ–ª–∏:", styles["Heading"]))
            class_data = [["–ö–ª–∞—Å—Å", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"]]
            for cls, cnt in result["prediction_counts"].items():
                class_data.append([str(cls), str(cnt)])
            class_table = Table(class_data, hAlign="LEFT", colWidths=[200, 200])
            class_table.setStyle(
                TableStyle(
                    [
                        ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                        ("BACKGROUND", (0, 0), (-1, 0), colors.lightblue),
                        ("FONTNAME", (0, 0), (-1, -1), "DejaVuSans"),
                        ("FONTSIZE", (0, 0), (-1, -1), 10),
                        ("ALIGN", (0, 0), (-1, 0), "CENTER"),
                        ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                    ]
                )
            )
            story.append(class_table)
            story.append(Spacer(1, 12))

        if result.get("average_probabilities"):
            story.append(PageBreak())
            story.append(Paragraph("–°—Ä–µ–¥–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:", styles["Heading"]))
            prob_rows = [["–ö–ª–∞—Å—Å", "–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å"]]
            for label, prob in result["average_probabilities"].items():
                prob_rows.append([truncate_text(str(label)), f"{prob * 100:.2f}%"])

            prob_table = Table(
                [[Paragraph(str(cell), styles["Body"]) for cell in row] for row in prob_rows],
                hAlign="LEFT",
                colWidths=[200, 250],
            )
            prob_table.setStyle(
                TableStyle(
                    [
                        ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                        ("BACKGROUND", (0, 0), (-1, 0), colors.lightblue),
                        ("ALIGN", (0, 0), (-1, 0), "CENTER"),
                        ("VALIGN", (0, 0), (-1, -1), "TOP"),
                    ]
                )
            )
            story.append(prob_table)
            story.append(Spacer(1, 12))

        return story

    def build_single_prediction_story(prediction: Dict[str, Any], styles, available_width: float) -> List[Any]:
        story: List[Any] = []
        story.append(Paragraph("–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏", styles["TitleRu"]))
        story.append(Spacer(1, 16))

        summary_rows: List[List[str]] = []
        generated_at = prediction.get("generated_at")
        if generated_at:
            try:
                timestamp = datetime.fromisoformat(generated_at)
                summary_rows.append(["–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –ø—Ä–æ–≥–Ω–æ–∑–∞", timestamp.strftime("%d.%m.%Y %H:%M:%S")])
            except ValueError:
                summary_rows.append(["–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –ø—Ä–æ–≥–Ω–æ–∑–∞", truncate_text(str(generated_at))])

        source_label = "CSV" if prediction.get("source") == "csv" else "–§–æ—Ä–º–∞"
        summary_rows.append(["–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö", source_label])
        summary_rows.append(["–û–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è –∞–Ω–æ–º–∞–ª–∏—è", truncate_text(str(prediction.get("anomaly_label", "-")))])

        predicted_probability = prediction.get("predicted_probability")
        if predicted_probability is not None:
            summary_rows.append(["–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞", f"{predicted_probability * 100:.2f}%"])

        suspicious = prediction.get("suspicious")
        if suspicious is not None:
            suspicious_text = "–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è" if suspicious else "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
            summary_rows.append(["–°—Ç–∞—Ç—É—Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏", suspicious_text])

        suspicious_prob = prediction.get("suspicious_probability")
        if suspicious_prob is not None:
            summary_rows.append(["–£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞", f"{suspicious_prob * 100:.1f}%"])

        summary_table = Table(
            [[Paragraph(str(cell), styles["Body"]) for cell in row] for row in [["–ü–∞—Ä–∞–º–µ—Ç—Ä", "–ó–Ω–∞—á–µ–Ω–∏–µ"], *summary_rows]],
            hAlign="LEFT",
            colWidths=[220, 230],
        )
        summary_table.setStyle(
            TableStyle(
                [
                    ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                    ("BACKGROUND", (0, 0), (-1, 0), colors.lightblue),
                    ("ALIGN", (0, 0), (-1, 0), "CENTER"),
                    ("VALIGN", (0, 0), (-1, -1), "TOP"),
                ]
            )
        )
        story.append(summary_table)
        story.append(Spacer(1, 14))

        prob_items = prediction.get("anomaly_probabilities")
        if prob_items:
            story.append(Paragraph("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Å–∞–º:", styles["Heading"]))
            prob_rows = [["–ö–ª–∞—Å—Å", "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å"]]
            for item in prob_items:
                prob_rows.append(
                    [
                        truncate_text(str(item.get("label", "-"))),
                        f"{float(item.get("probability", 0.0)) * 100:.2f}%",
                    ]
                )
            prob_table = Table(
                [[Paragraph(str(cell), styles["Body"]) for cell in row] for row in prob_rows],
                hAlign="LEFT",
                colWidths=[220, 150],
            )
            prob_table.setStyle(
                TableStyle(
                    [
                        ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                        ("BACKGROUND", (0, 0), (-1, 0), colors.lightblue),
                        ("ALIGN", (0, 0), (-1, 0), "CENTER"),
                        ("VALIGN", (0, 0), (-1, -1), "TOP"),
                    ]
                )
            )
            story.append(prob_table)
            story.append(Spacer(1, 14))

        preview_html = prediction.get("preview_vertical_html") or prediction.get("preview_html")
        if preview_html:
            story.append(Paragraph("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:", styles["Heading"]))
            soup = BeautifulSoup(preview_html, "html.parser")
            record_pairs: List[List[Paragraph]] = []

            vertical_rows = soup.select("table.preview-table--vertical tbody tr")
            if vertical_rows:
                for row in vertical_rows:
                    cells = row.find_all(["th", "td"])
                    if not cells:
                        continue
                    header_text = truncate_text(cells[0].get_text(strip=True), 40)
                    value_text = "-"
                    if len(cells) > 1:
                        value_text = truncate_text(cells[1].get_text(strip=True), 80)
                    record_pairs.append(
                        [
                            Paragraph(header_text, styles["Body"]),
                            Paragraph(value_text, styles["Body"]),
                        ]
                    )
            else:
                header_cells = soup.select("thead tr th")
                row_cells = soup.select("tbody tr:first-child td")

                if not header_cells:
                    rows = soup.find_all("tr")
                    if rows:
                        potential_headers = rows[0].find_all("th")
                        if potential_headers:
                            header_cells = potential_headers
                            if len(rows) > 1:
                                row_cells = rows[1].find_all("td")

                if header_cells and row_cells:
                    for index, header_cell in enumerate(header_cells):
                        header_text = truncate_text(header_cell.get_text(strip=True), 40)
                        value_text = truncate_text(
                            row_cells[index].get_text(strip=True) if index < len(row_cells) else "-", 80
                        )
                        record_pairs.append(
                            [
                                Paragraph(header_text, styles["Body"]),
                                Paragraph(value_text, styles["Body"]),
                            ]
                        )
                else:
                    # fallback: treat each table cell sequentially
                    first_row = soup.find("tr")
                    if first_row:
                        cells = [
                            truncate_text(cell.get_text(strip=True), 80)
                            for cell in first_row.find_all(["th", "td"])
                        ]
                        for idx, cell_value in enumerate(cells, start=1):
                            record_pairs.append(
                                [
                                    Paragraph(f"–ü—Ä–∏–∑–Ω–∞–∫ {idx}", styles["Body"]),
                                    Paragraph(cell_value or "-", styles["Body"]),
                                ]
                            )

            if record_pairs:
                preview_table = Table(
                    [[Paragraph("–ü—Ä–∏–∑–Ω–∞–∫", styles["Body"]), Paragraph("–ó–Ω–∞—á–µ–Ω–∏–µ", styles["Body"])], *record_pairs],
                    colWidths=[available_width * 0.35, available_width * 0.6],
                    hAlign="LEFT",
                )
                preview_table.setStyle(
                    TableStyle(
                        [
                            ("GRID", (0, 0), (-1, -1), 0.25, colors.grey),
                            ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
                            ("VALIGN", (0, 0), (-1, -1), "TOP"),
                            ("LEFTPADDING", (0, 0), (-1, -1), 3),
                            ("RIGHTPADDING", (0, 0), (-1, -1), 3),
                            ("TOPPADDING", (0, 0), (-1, -1), 3),
                            ("BOTTOMPADDING", (0, 0), (-1, -1), 3),
                        ]
                    )
                )
                story.append(preview_table)
                story.append(Spacer(1, 14))

        prep_notes = prediction.get("preprocessing_notes") or []
        if prep_notes:
            story.append(Paragraph("–ü—Ä–∏–º–µ—á–∞–Ω–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:", styles["Heading"]))
            for note in prep_notes:
                story.append(Paragraph(f"‚Ä¢ {truncate_text(str(note), 120)}", styles["Body"]))
            story.append(Spacer(1, 12))

        recommendation: Optional[str] = None
        if suspicious is not None:
            if suspicious:
                recommendation = (
                    "FalcoNS"
                )
            else:
                recommendation = "FalcoNS"
        if recommendation:
            story.append(Paragraph(recommendation, styles["Body"]))

        return story

    @app.get("/export-pdf")
    async def export_pdf(request: Request, scope: str = Query("dataset")):
        font_error = ensure_pdf_font()
        if font_error:
            return font_error

        store = request.app.state.store
        content: Optional[List[Any]] = None
        filename = "dataset_analysis.pdf"

        tmp_pdf = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
        doc = SimpleDocTemplate(
            tmp_pdf.name,
            pagesize=A4,
            rightMargin=30,
            leftMargin=30,
            topMargin=30,
            bottomMargin=30,
        )

        styles = getSampleStyleSheet()
        styles.add(
            ParagraphStyle(
                name="TitleRu",
                fontName="DejaVuSans",
                fontSize=18,
                leading=22,
                alignment=1,
                textColor=colors.darkblue,
            )
        )
        styles.add(
            ParagraphStyle(
                name="Heading",
                fontName="DejaVuSans",
                fontSize=14,
                leading=18,
                textColor=colors.HexColor("#1f2937"),
            )
        )
        styles.add(ParagraphStyle(name="Body", fontName="DejaVuSans", fontSize=10, leading=14))

        if scope == "single":
            prediction = store.last_prediction
            if not prediction:
                return HTMLResponse("<h3>–ù–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞</h3>")
            content = build_single_prediction_story(prediction, styles, doc.width)
            filename = "single_prediction.pdf"
        else:
            result = store.last_analysis
            if not result:
                return HTMLResponse("<h3>–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞</h3>")
            content = build_dataset_story(result, styles, doc.width)

        doc.build(content)
        return FileResponse(tmp_pdf.name, filename=filename, media_type="application/pdf")
